# A voir : The Future of Deep Learning Research by Siraj Raval https://www.youtube.com/watch?v=WTnxE0wjZaM

Notes sur la vidÃ©os 

```
> From the beginning to 15'23 : Back Propagation 

-Linear algebra allows the multiplcation groups of numbers. Matrix. 
-Foward propagation : Input * weight + bias then activate 
-Backprobagation 
	-derivative 
	-partial derivative 
	-chain rule 
find weights and bias values to find the minimum of the parabola. 
Key different between the brain and neurals networks. How the mind works? Steven Pinker 



> From 21' to 29': Classification versus Generation , the most popular Solution. 

-Classification and Generation = Discovery and Creativity 

-Algorithm most popular for Classification : 
Clustering (k-means , PCA) 
Auto-Encoder 

-Algorithm most popular for Generation : 
GAN Generative Adversarial Network.
Variational Autoencoder inside of the layer random variable. 
DNC Differientiable Neuron Computer seprating the memory from the neural itself. 
But is with Backpropagation!!!!!



> From 29' to the end of the video : 

7 Research Directions : Use more unsupervised learning and more reinforcement learning 
1-Bayesian Deep Learning
Prior assumption of how the world is working. 
2-Spike Timing Dependent Plasticity  
Take timing in consideration 
3-Self Organizing Map 
Do not use back propagation 
4-Synthetic Gradients (Deepmind) 
5-Evolutionary strategy (OpenAI)
6-Moar Reinforcement learning (AlphaGo)
7-Better Hardware (Neuromorphics chips IBM, TPU tensor Processing Units)
Explotation and Exploration.....focus on more on Exploration!!!! 

```

